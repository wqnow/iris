{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“cifar10_1.ipynb”的副本",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wqnow/iris/blob/master/%E2%80%9Ccifar10_1_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CpyWp0W37BtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6083
        },
        "outputId": "3f26a340-dfc3-482f-c250-883ee898d0b5"
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Using Dataset API and tf.image can obtain the best performance.\"\"\"\n",
        "!pip install tensorlayer\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "ggdrive = GoogleDrive(gauth)\n",
        "\n",
        "import time\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import InputLayer, Conv2d, BatchNormLayer, MaxPool2d, FlattenLayer, DenseLayer\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n",
        "batch_size = 128\n",
        "n_epoch = 50\n",
        "learning_rate = 0.0001\n",
        "n_step_epoch = int(len(y_train) / batch_size)\n",
        "n_step = n_epoch * n_step_epoch\n",
        "shuffle_buffer_size = 100\n",
        "\n",
        "def model_batch_norm(x_crop, y_, is_train, reuse):\n",
        "    W_init = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    b_init2 = tf.constant_initializer(value=0.1)\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(x_crop, name='input')\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME', W_init=W_init, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, decay=0.99, is_train=is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME', W_init=W_init, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, decay=0.99, is_train=is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n",
        "        net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')\n",
        "        y = net.outputs\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y, y_, name='cost')\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.cast(tf.argmax(y, 1), tf.int32), y_)\n",
        "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, acc\n",
        "\n",
        "\n",
        "def generator_train():\n",
        "    inputs = X_train\n",
        "    targets = y_train\n",
        "    if len(inputs) != len(targets):\n",
        "        raise AssertionError(\"The length of inputs and targets should be equal\")\n",
        "    for _input, _target in zip(inputs, targets):\n",
        "        # yield _input.encode('utf-8'), _target.encode('utf-8')\n",
        "        yield _input, _target\n",
        "\n",
        "\n",
        "def generator_test():\n",
        "    inputs = X_test\n",
        "    targets = y_test\n",
        "    if len(inputs) != len(targets):\n",
        "        raise AssertionError(\"The length of inputs and targets should be equal\")\n",
        "    for _input, _target in zip(inputs, targets):\n",
        "        # yield _input.encode('utf-8'), _target.encode('utf-8')\n",
        "        yield _input, _target\n",
        "\n",
        "\n",
        "def _map_fn_train(img, target):\n",
        "    # 1. Randomly crop a [height, width] section of the image.\n",
        "    img = tf.random_crop(img, [24, 24, 3])\n",
        "    # 2. Randomly flip the image horizontally.\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    # 3. Randomly change brightness.\n",
        "    img = tf.image.random_brightness(img, max_delta=63)\n",
        "    # 4. Randomly change contrast.\n",
        "    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n",
        "    # 5. Subtract off the mean and divide by the variance of the pixels.\n",
        "    img = tf.image.per_image_standardization(img)\n",
        "    target = tf.reshape(target, ())\n",
        "    return img, target\n",
        "\n",
        "\n",
        "def _map_fn_test(img, target):\n",
        "    # 1. Crop the central [height, width] of the image.\n",
        "    img = tf.image.resize_image_with_crop_or_pad(img, 24, 24)\n",
        "    # 2. Subtract off the mean and divide by the variance of the pixels.\n",
        "    img = tf.image.per_image_standardization(img)\n",
        "    img = tf.reshape(img, (24, 24, 3))\n",
        "    target = tf.reshape(target, ())\n",
        "    return img, target\n",
        "\n",
        "\n",
        "# dataset API and augmentation\n",
        "ds = tf.data.Dataset().from_generator(generator_train, output_types=(tf.float32,\n",
        "                                                                     tf.int32))  # , output_shapes=((24, 24, 3), (1)))\n",
        "ds = ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n",
        "ds = ds.repeat(n_epoch)\n",
        "ds = ds.shuffle(shuffle_buffer_size)\n",
        "ds = ds.prefetch(buffer_size=4096)\n",
        "ds = ds.batch(batch_size)\n",
        "value = ds.make_one_shot_iterator().get_next()\n",
        "\n",
        "ds = tf.data.Dataset().from_generator(generator_test, output_types=(tf.float32,\n",
        "                                                                    tf.int32))  # , output_shapes=((24, 24, 3), (1)))\n",
        "ds = ds.shuffle(shuffle_buffer_size)\n",
        "ds = ds.map(_map_fn_test, num_parallel_calls=multiprocessing.cpu_count())\n",
        "ds = ds.repeat(n_epoch)\n",
        "ds = ds.prefetch(buffer_size=4096)\n",
        "ds = ds.batch(batch_size)\n",
        "value_test = ds.make_one_shot_iterator().get_next()\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        net, cost, acc, = model_batch_norm(value[0], value[1], True, False)\n",
        "        _, cost_test, acc_test = model_batch_norm(value_test[0], value_test[1], False, True)\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    print('   learning_rate: %f' % learning_rate)\n",
        "    print('   batch_size: %d' % batch_size)\n",
        "    print('   n_epoch: %d, step in an epoch: %d, total n_step: %d' % (n_epoch, n_step_epoch, n_step))\n",
        "\n",
        "    step = 0\n",
        "    epoch = 0\n",
        "    train_loss, train_acc, n_batch = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "    while step < n_step:\n",
        "        # train one batch\n",
        "        err, ac, _ = sess.run([cost, acc, train_op])\n",
        "        step += 1\n",
        "        train_loss += err\n",
        "        train_acc += ac\n",
        "        n_batch += 1\n",
        "        # one epoch finished, start evaluation\n",
        "        if (step % n_step_epoch) == 0:\n",
        "            print(\n",
        "                \"Epoch %d : Step %d-%d of %d took %fs\" %\n",
        "                (epoch, step, step + n_step_epoch, n_step, time.time() - start_time)\n",
        "            )\n",
        "            print(\"   train loss: %f\" % (train_loss / n_batch))\n",
        "            print(\"   train acc: %f\" % (train_acc / n_batch))\n",
        "\n",
        "            test_loss, test_acc, n_batch = 0, 0, 0\n",
        "            for _ in range(int(len(y_test) / batch_size)):\n",
        "                err, ac = sess.run([cost_test, acc_test])\n",
        "                test_loss += err\n",
        "                test_acc += ac\n",
        "                n_batch += 1\n",
        "            print(\"   test loss: %f\" % (test_loss / n_batch))\n",
        "            print(\"   test acc: %f\" % (test_acc / n_batch))\n",
        "            train_loss, train_acc, n_batch = 0, 0, 0\n",
        "            epoch += 1\n",
        "            start_time = time.time()\n",
        "            # save model\n",
        "            if (step % (n_step_epoch * 5)) == 0:\n",
        "                tl.files.save_npz(net.all_params, name='model.npz', sess=sess)\n",
        "                filename = 'model_' + str(step) + '.npz'\n",
        "                file1 = ggdrive.CreateFile({'title': filename})  # Create GoogleDriveFile instance with title 'Hello.txt'.\n",
        "                file1.SetContentFile('model.npz') # Set content of the file from given string.\n",
        "                file1.Upload()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.6/dist-packages (1.11.0)\n",
            "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.10.11)\n",
            "Requirement already satisfied: tqdm<4.28,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.27.0)\n",
            "Requirement already satisfied: imageio<2.5,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.4.1)\n",
            "Requirement already satisfied: scikit-image<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.14.1)\n",
            "Requirement already satisfied: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.1.0)\n",
            "Requirement already satisfied: matplotlib<3.1,>=2.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.0.1)\n",
            "Requirement already satisfied: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.38.0)\n",
            "Requirement already satisfied: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (4.2.5)\n",
            "Requirement already satisfied: scikit-learn<0.21,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.19.2)\n",
            "Requirement already satisfied: requests<2.20,>=2.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.19.1)\n",
            "Requirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<2.5,>=2.3->tensorlayer) (5.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.11.0)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.20.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (1.0.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer) (0.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.1,>=2.2->tensorlayer) (1.0.1)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2018.10.15)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer) (1.22)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer) (0.9.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer) (4.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.1,>=2.2->tensorlayer) (40.5.0)\n",
            "0\n",
            "[TL] Load or Download cifar10 > data/cifar10\n",
            "0a\n",
            "1\n",
            "2\n",
            "[TL] InputLayer  model/input: (?, 24, 24, 3)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 0.990000 epsilon: 0.000010 act: relu is_train: True\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 0.990000 epsilon: 0.000010 act: relu is_train: True\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 2304\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (2304, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "[TL] InputLayer  model/input: (?, 24, 24, 3)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 0.990000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 0.990000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 2304\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (2304, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "3\n",
            "   learning_rate: 0.000100\n",
            "   batch_size: 128\n",
            "   n_epoch: 50, step in an epoch: 390, total n_step: 19500\n",
            "Epoch 0 : Step 390-780 of 19500 took 14.914857s\n",
            "   train loss: 3.612077\n",
            "   train acc: 0.389423\n",
            "   test loss: 3.056939\n",
            "   test acc: 0.483073\n",
            "Epoch 1 : Step 780-1170 of 19500 took 14.287952s\n",
            "   train loss: 2.807876\n",
            "   train acc: 0.500801\n",
            "   test loss: 2.553665\n",
            "   test acc: 0.538061\n",
            "Epoch 2 : Step 1170-1560 of 19500 took 14.201054s\n",
            "   train loss: 2.426057\n",
            "   train acc: 0.544832\n",
            "   test loss: 2.281436\n",
            "   test acc: 0.568009\n",
            "Epoch 3 : Step 1560-1950 of 19500 took 14.226476s\n",
            "   train loss: 2.187331\n",
            "   train acc: 0.567348\n",
            "   test loss: 2.012566\n",
            "   test acc: 0.604968\n",
            "Epoch 4 : Step 1950-2340 of 19500 took 14.208456s\n",
            "   train loss: 1.995963\n",
            "   train acc: 0.587680\n",
            "   test loss: 1.840673\n",
            "   test acc: 0.622296\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 5 : Step 2340-2730 of 19500 took 15.337356s\n",
            "   train loss: 1.852709\n",
            "   train acc: 0.600861\n",
            "   test loss: 1.718149\n",
            "   test acc: 0.631210\n",
            "Epoch 6 : Step 2730-3120 of 19500 took 14.122918s\n",
            "   train loss: 1.736880\n",
            "   train acc: 0.611558\n",
            "   test loss: 1.583297\n",
            "   test acc: 0.654647\n",
            "Epoch 7 : Step 3120-3510 of 19500 took 14.094214s\n",
            "   train loss: 1.636425\n",
            "   train acc: 0.624659\n",
            "   test loss: 1.487439\n",
            "   test acc: 0.667368\n",
            "Epoch 8 : Step 3510-3900 of 19500 took 14.192154s\n",
            "   train loss: 1.547549\n",
            "   train acc: 0.635296\n",
            "   test loss: 1.431266\n",
            "   test acc: 0.667067\n",
            "Epoch 9 : Step 3900-4290 of 19500 took 14.146363s\n",
            "   train loss: 1.474354\n",
            "   train acc: 0.645493\n",
            "   test loss: 1.333504\n",
            "   test acc: 0.689002\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 10 : Step 4290-4680 of 19500 took 14.817972s\n",
            "   train loss: 1.408145\n",
            "   train acc: 0.654427\n",
            "   test loss: 1.276121\n",
            "   test acc: 0.698417\n",
            "Epoch 11 : Step 4680-5070 of 19500 took 14.146943s\n",
            "   train loss: 1.358713\n",
            "   train acc: 0.659014\n",
            "   test loss: 1.229579\n",
            "   test acc: 0.701623\n",
            "Epoch 12 : Step 5070-5460 of 19500 took 14.034586s\n",
            "   train loss: 1.308245\n",
            "   train acc: 0.667768\n",
            "   test loss: 1.170894\n",
            "   test acc: 0.716346\n",
            "Epoch 13 : Step 5460-5850 of 19500 took 14.141132s\n",
            "   train loss: 1.265891\n",
            "   train acc: 0.669551\n",
            "   test loss: 1.132774\n",
            "   test acc: 0.716647\n",
            "Epoch 14 : Step 5850-6240 of 19500 took 13.995140s\n",
            "   train loss: 1.232446\n",
            "   train acc: 0.675020\n",
            "   test loss: 1.116027\n",
            "   test acc: 0.714944\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 15 : Step 6240-6630 of 19500 took 14.937789s\n",
            "   train loss: 1.191011\n",
            "   train acc: 0.683313\n",
            "   test loss: 1.097548\n",
            "   test acc: 0.707833\n",
            "Epoch 16 : Step 6630-7020 of 19500 took 13.986498s\n",
            "   train loss: 1.164546\n",
            "   train acc: 0.688341\n",
            "   test loss: 1.032943\n",
            "   test acc: 0.731971\n",
            "Epoch 17 : Step 7020-7410 of 19500 took 13.815437s\n",
            "   train loss: 1.137679\n",
            "   train acc: 0.691206\n",
            "   test loss: 1.036962\n",
            "   test acc: 0.730369\n",
            "Epoch 18 : Step 7410-7800 of 19500 took 13.926216s\n",
            "   train loss: 1.107716\n",
            "   train acc: 0.695112\n",
            "   test loss: 1.009346\n",
            "   test acc: 0.729367\n",
            "Epoch 19 : Step 7800-8190 of 19500 took 13.939583s\n",
            "   train loss: 1.088817\n",
            "   train acc: 0.698878\n",
            "   test loss: 0.973478\n",
            "   test acc: 0.735978\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 20 : Step 8190-8580 of 19500 took 14.548311s\n",
            "   train loss: 1.067920\n",
            "   train acc: 0.704347\n",
            "   test loss: 0.943963\n",
            "   test acc: 0.751803\n",
            "Epoch 21 : Step 8580-8970 of 19500 took 13.778221s\n",
            "   train loss: 1.043943\n",
            "   train acc: 0.707171\n",
            "   test loss: 0.958138\n",
            "   test acc: 0.742989\n",
            "Epoch 22 : Step 8970-9360 of 19500 took 13.939466s\n",
            "   train loss: 1.031537\n",
            "   train acc: 0.709475\n",
            "   test loss: 0.940233\n",
            "   test acc: 0.746795\n",
            "Epoch 23 : Step 9360-9750 of 19500 took 13.927217s\n",
            "   train loss: 1.016148\n",
            "   train acc: 0.710777\n",
            "   test loss: 0.907528\n",
            "   test acc: 0.754407\n",
            "Epoch 24 : Step 9750-10140 of 19500 took 13.959665s\n",
            "   train loss: 0.995576\n",
            "   train acc: 0.715865\n",
            "   test loss: 0.920699\n",
            "   test acc: 0.750501\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 25 : Step 10140-10530 of 19500 took 14.594301s\n",
            "   train loss: 0.986060\n",
            "   train acc: 0.718990\n",
            "   test loss: 0.919485\n",
            "   test acc: 0.742087\n",
            "Epoch 26 : Step 10530-10920 of 19500 took 13.980597s\n",
            "   train loss: 0.968507\n",
            "   train acc: 0.722256\n",
            "   test loss: 0.922268\n",
            "   test acc: 0.743690\n",
            "Epoch 27 : Step 10920-11310 of 19500 took 13.954078s\n",
            "   train loss: 0.957319\n",
            "   train acc: 0.724119\n",
            "   test loss: 0.882981\n",
            "   test acc: 0.752504\n",
            "Epoch 28 : Step 11310-11700 of 19500 took 13.994331s\n",
            "   train loss: 0.945590\n",
            "   train acc: 0.728526\n",
            "   test loss: 0.876919\n",
            "   test acc: 0.750801\n",
            "Epoch 29 : Step 11700-12090 of 19500 took 13.986861s\n",
            "   train loss: 0.935883\n",
            "   train acc: 0.729307\n",
            "   test loss: 0.866945\n",
            "   test acc: 0.756010\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 30 : Step 12090-12480 of 19500 took 15.017578s\n",
            "   train loss: 0.926672\n",
            "   train acc: 0.729647\n",
            "   test loss: 0.882945\n",
            "   test acc: 0.748197\n",
            "Epoch 31 : Step 12480-12870 of 19500 took 14.056422s\n",
            "   train loss: 0.916913\n",
            "   train acc: 0.733053\n",
            "   test loss: 0.894602\n",
            "   test acc: 0.739483\n",
            "Epoch 32 : Step 12870-13260 of 19500 took 13.900017s\n",
            "   train loss: 0.909276\n",
            "   train acc: 0.734515\n",
            "   test loss: 0.876905\n",
            "   test acc: 0.747496\n",
            "Epoch 33 : Step 13260-13650 of 19500 took 13.912495s\n",
            "   train loss: 0.899304\n",
            "   train acc: 0.736679\n",
            "   test loss: 0.857848\n",
            "   test acc: 0.753606\n",
            "Epoch 34 : Step 13650-14040 of 19500 took 14.093824s\n",
            "   train loss: 0.892082\n",
            "   train acc: 0.741827\n",
            "   test loss: 0.805429\n",
            "   test acc: 0.772436\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 35 : Step 14040-14430 of 19500 took 14.660908s\n",
            "   train loss: 0.888105\n",
            "   train acc: 0.740966\n",
            "   test loss: 0.859631\n",
            "   test acc: 0.746895\n",
            "Epoch 36 : Step 14430-14820 of 19500 took 13.936052s\n",
            "   train loss: 0.872611\n",
            "   train acc: 0.743770\n",
            "   test loss: 0.824408\n",
            "   test acc: 0.764423\n",
            "Epoch 37 : Step 14820-15210 of 19500 took 14.027627s\n",
            "   train loss: 0.863470\n",
            "   train acc: 0.748157\n",
            "   test loss: 0.816045\n",
            "   test acc: 0.763321\n",
            "Epoch 38 : Step 15210-15600 of 19500 took 13.947622s\n",
            "   train loss: 0.866824\n",
            "   train acc: 0.747616\n",
            "   test loss: 0.786612\n",
            "   test acc: 0.774539\n",
            "Epoch 39 : Step 15600-15990 of 19500 took 13.896434s\n",
            "   train loss: 0.850789\n",
            "   train acc: 0.749299\n",
            "   test loss: 0.817711\n",
            "   test acc: 0.758814\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 40 : Step 15990-16380 of 19500 took 14.759681s\n",
            "   train loss: 0.849721\n",
            "   train acc: 0.749139\n",
            "   test loss: 0.810840\n",
            "   test acc: 0.764724\n",
            "Epoch 41 : Step 16380-16770 of 19500 took 13.982223s\n",
            "   train loss: 0.844407\n",
            "   train acc: 0.751923\n",
            "   test loss: 0.800821\n",
            "   test acc: 0.763622\n",
            "Epoch 42 : Step 16770-17160 of 19500 took 13.985544s\n",
            "   train loss: 0.836258\n",
            "   train acc: 0.754067\n",
            "   test loss: 0.825283\n",
            "   test acc: 0.756110\n",
            "Epoch 43 : Step 17160-17550 of 19500 took 14.009806s\n",
            "   train loss: 0.831675\n",
            "   train acc: 0.755449\n",
            "   test loss: 0.845428\n",
            "   test acc: 0.754507\n",
            "Epoch 44 : Step 17550-17940 of 19500 took 13.970661s\n",
            "   train loss: 0.829779\n",
            "   train acc: 0.757292\n",
            "   test loss: 0.804322\n",
            "   test acc: 0.768129\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n",
            "Epoch 45 : Step 17940-18330 of 19500 took 14.743761s\n",
            "   train loss: 0.821251\n",
            "   train acc: 0.760056\n",
            "   test loss: 0.803461\n",
            "   test acc: 0.769030\n",
            "Epoch 46 : Step 18330-18720 of 19500 took 14.036491s\n",
            "   train loss: 0.819479\n",
            "   train acc: 0.758193\n",
            "   test loss: 0.806365\n",
            "   test acc: 0.765224\n",
            "Epoch 47 : Step 18720-19110 of 19500 took 14.022481s\n",
            "   train loss: 0.808673\n",
            "   train acc: 0.763782\n",
            "   test loss: 0.802795\n",
            "   test acc: 0.772436\n",
            "Epoch 48 : Step 19110-19500 of 19500 took 14.206335s\n",
            "   train loss: 0.805156\n",
            "   train acc: 0.763281\n",
            "   test loss: 0.763853\n",
            "   test acc: 0.781751\n",
            "Epoch 49 : Step 19500-19890 of 19500 took 14.175081s\n",
            "   train loss: 0.800624\n",
            "   train acc: 0.764984\n",
            "   test loss: 0.739473\n",
            "   test acc: 0.790865\n",
            "[TL] [*] Saving TL params into model.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g6M6fNDFNGq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = 'model_' + str(step) + '.npz'\n",
        "file1 = ggdrive.CreateFile({'title': filename})  # Create GoogleDriveFile instance with title 'Hello.txt'.\n",
        "file1.SetContentFile('model.npz') # Set content of the file from given string.\n",
        "file1.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rflyz3fRGXqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "终于搞定保存到谷歌硬盘里了。下面要解决读取model文件继续训练。"
      ]
    }
  ]
}